<!DOCTYPE html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.60" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://ppai.top/ai-guides/tutorial/hello-llm/13.%E8%AE%B0%E5%BF%86%E7%AD%96%E7%95%A5.html"><meta property="og:site_name" content="Helllo LLM Guides"><meta property="og:title" content="第 8 章：记忆策略的工程化选择"><meta property="og:description" content="在前两章中，我们已经完成了两个关键决策： 否定“无限上下文”的幻想：明确上下文窗口存在硬性约束，无法承载无限制的对话信息； 接受“信息必须被分层管理”：将信息划分为不变约束、会话状态、瞬时上下文，优先保障高优先级信息的有效性。 接下来，我们需要面对一个更系统级的问题，当然这也是很多小伙伴在具体落地时，最容易困惑的地方 “记忆”是否应该只有一种实现方式？ 答案显然是否定的，就像人类会用“瞬时回忆”记住刚说的话、用“短期记忆”记住当天的任务、用“长期记忆”记住过往的经验一样，大模型应用的“记忆”也需要分层设计"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2026-02-04T07:10:37.000Z"><meta property="article:tag" content="LLM"><meta property="article:published_time" content="2025-12-30T14:55:07.000Z"><meta property="article:modified_time" content="2026-02-04T07:10:37.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"第 8 章：记忆策略的工程化选择","image":[""],"datePublished":"2025-12-30T14:55:07.000Z","dateModified":"2026-02-04T07:10:37.000Z","author":[]}</script><link rel="icon" href="/ai-guides/favicon.ico"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><meta name="robots" content="all"><meta name="author" content="一灰灰blog"><meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate"><meta http-equiv="Pragma" content="no-cache"><meta http-equiv="Expires" content="0"><meta name="keywords" content="GitHub, SpringAI, 大模型, LLM, 人工智能, AI, LangChain, LangGraph, RAG, RPA, FunctionCalling, 智能体"><meta name="apple-mobile-web-app-capable" content="yes"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?d09f8c5c03cb6eca0190078252f46f64";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><link rel="stylesheet" href="/iconfont/iconfont.css"><title>第 8 章：记忆策略的工程化选择 | Helllo LLM Guides</title><meta name="description" content="在前两章中，我们已经完成了两个关键决策： 否定“无限上下文”的幻想：明确上下文窗口存在硬性约束，无法承载无限制的对话信息； 接受“信息必须被分层管理”：将信息划分为不变约束、会话状态、瞬时上下文，优先保障高优先级信息的有效性。 接下来，我们需要面对一个更系统级的问题，当然这也是很多小伙伴在具体落地时，最容易困惑的地方 “记忆”是否应该只有一种实现方式？ 答案显然是否定的，就像人类会用“瞬时回忆”记住刚说的话、用“短期记忆”记住当天的任务、用“长期记忆”记住过往的经验一样，大模型应用的“记忆”也需要分层设计">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.querySelector("html").setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/ai-guides/assets/style-35a30649.css" as="style"><link rel="stylesheet" href="/ai-guides/assets/style-35a30649.css">
    <link rel="modulepreload" href="/ai-guides/assets/app-f3f5ea18.js"><link rel="modulepreload" href="/ai-guides/assets/preload-helper-6113a707.js"><link rel="modulepreload" href="/ai-guides/assets/framework-2236aa4f.js"><link rel="modulepreload" href="/ai-guides/assets/13.记忆策略.html-b7e2d368.js"><link rel="modulepreload" href="/ai-guides/assets/13.记忆策略.html-50fa6246.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container has-toc"><!--[--><header class="navbar" id="navbar"><div class="navbar-start"><button class="toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><a href="/ai-guides/" class="brand"><img class="logo" src="/ai-guides/favicon.ico" alt="Helllo LLM Guides"><!----><span class="site-name hide-in-pad">Helllo LLM Guides</span></a><!--[--><!----><!--]--></div><div class="navbar-center"><!--[--><!----><!--]--><nav class="nav-links"><div class="nav-item hide-in-mobile"><a href="/ai-guides/" class="nav-link" aria-label="主页"><span class="font-icon icon iconfont icon-home" style=""></span>主页<!----></a></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button class="dropdown-title" type="button" aria-label="LLM开发手册"><span class="title"><span class="font-icon icon iconfont icon-diagram" style=""></span>LLM开发手册</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a href="/ai-guides/tutorial/" class="nav-link active" aria-label="LLM教程"><span class="font-icon icon iconfont icon-docs" style=""></span>LLM教程<!----></a></li><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>Hello LLM</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/ai-guides/tutorial/hello-llm/" class="nav-link active" aria-label="LLM应用开发"><span class="font-icon icon iconfont icon-exercise" style=""></span>LLM应用开发<!----></a></li><li class="dropdown-subitem"><a href="/ai-guides/tutorial/hello-agent/" class="nav-link" aria-label="Agent篇"><span class="font-icon icon iconfont icon-router" style=""></span>Agent篇<!----></a></li></ul></li><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>AI Coding</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/ai-guides/tutorial/ai-coding/" class="nav-link" aria-label="AI编程实战"><span class="font-icon icon iconfont icon-computer" style=""></span>AI编程实战<!----></a></li></ul></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button class="dropdown-title" type="button" aria-label="LLM应用开发"><span class="title"><span class="font-icon icon iconfont icon-material" style=""></span>LLM应用开发</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a href="/ai-guides/ai-dev/" class="nav-link" aria-label="SpringAI"><span class="font-icon icon iconfont icon-material" style=""></span>SpringAI<!----></a></li><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>SpringAI</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/ai-guides/ai-dev/%E5%9F%BA%E7%A1%80%E7%AF%87/" class="nav-link" aria-label="基础篇"><!---->基础篇<!----></a></li><li class="dropdown-subitem"><a href="/ai-guides/ai-dev/%E8%BF%9B%E9%98%B6%E7%AF%87/" class="nav-link" aria-label="进阶篇"><!---->进阶篇<!----></a></li><li class="dropdown-subitem"><a href="/ai-guides/ai-dev/%E5%BA%94%E7%94%A8%E7%AF%87/" class="nav-link" aria-label="应用篇"><!---->应用篇<!----></a></li><li class="dropdown-subitem"><a href="/ai-guides/ai-dev/%E6%BA%90%E7%A0%81%E7%AF%87/" class="nav-link" aria-label="源码篇"><!---->源码篇<!----></a></li></ul></li></ul></button></div></div><div class="nav-item hide-in-mobile"><a href="https://app.ppai.top/" rel="noopener noreferrer" target="_blank" aria-label="百宝箱" class="nav-link"><span class="font-icon icon iconfont icon-tool" style=""></span>百宝箱<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="nav-item hide-in-mobile"><a href="https://hhui.top/" rel="noopener noreferrer" target="_blank" aria-label="一灰灰技术分享" class="nav-link"><span class="font-icon icon iconfont icon-tool" style=""></span>一灰灰技术分享<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div></nav><!--[--><!----><!--]--></div><div class="navbar-end"><!--[--><!----><!--]--><!----><div class="nav-item"><a class="repo-link" href="https://github.com/liuyueyi/ai-guides" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!--[--><button class="search-pro-button" role="search" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg></button><!--]--><!--[--><!----><!--]--><button class="toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span class="button-container"><span class="button-top"></span><span class="button-middle"></span><span class="button-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside class="sidebar" id="sidebar"><!--[--><!----><!--]--><ul class="sidebar-links"><li><!--[--><a href="/ai-guides/tutorial/" class="nav-link sidebar-link sidebar-page" aria-label="LLM教程"><span class="font-icon icon iconfont icon-docs" style=""></span>LLM教程<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><span class="font-icon icon iconfont icon-write" style=""></span><span class="title">AiCoding</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable active"><span class="font-icon icon iconfont icon-define" style=""></span><span class="title">Hello LLM</span><span class="arrow down"></span></button><ul class="sidebar-links"><li><!--[--><a href="/ai-guides/tutorial/hello-llm/01.llm%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E6%95%99%E5%AD%A6%E7%89%88%E6%95%99%E7%A8%8B.html" class="nav-link sidebar-link sidebar-page" aria-label="LLM 应用开发是什么：零基础也可以读懂的科普文(半小时速成)"><!---->LLM 应用开发是什么：零基础也可以读懂的科普文(半小时速成)<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a href="/ai-guides/tutorial/hello-llm/02.LLM%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E6%95%99%E7%A8%8B%E5%A4%A7%E7%BA%B2.html" class="nav-link sidebar-link sidebar-page" aria-label="LLM应用开发进阶大纲"><!---->LLM应用开发进阶大纲<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a href="/ai-guides/tutorial/hello-llm/03.%E5%89%8D%E8%A8%80.html" class="nav-link sidebar-link sidebar-page" aria-label="前言"><!---->前言<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a href="/ai-guides/tutorial/hello-llm/04.LLM%E5%88%B0%E5%BA%95%E5%9C%A8%E5%81%9A%E4%BB%80%E4%B9%88.html" class="nav-link sidebar-link sidebar-page" aria-label="第 1 章：LLM 到底在做什么？"><!---->第 1 章：LLM 到底在做什么？<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a href="/ai-guides/tutorial/hello-llm/05.LLM%E5%8F%82%E6%95%B0%E5%86%B3%E7%AD%96.html" class="nav-link sidebar-link sidebar-page" aria-label="第 2 章：模型不是重点，参数才是你真正的控制面板"><!---->第 2 章：模型不是重点，参数才是你真正的控制面板<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a href="/ai-guides/tutorial/hello-llm/06.%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%20Prompt%E5%B7%A5%E7%A8%8B.html" class="nav-link sidebar-link sidebar-page" aria-label="第二部分｜Prompt 工程：从“写提示词”到“设计约束系统”（以企业知识库助手为主线）"><!---->第二部分｜Prompt 工程：从“写提示词”到“设计约束系统”（以企业知识库助手为主线）<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a href="/ai-guides/tutorial/hello-llm/07.Prompt%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%A4%B1%E8%B4%A5.html" class="nav-link sidebar-link sidebar-page" aria-label="第 3 章：Prompt 为什么会失败？"><!---->第 3 章：Prompt 为什么会失败？<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a href="/ai-guides/tutorial/hello-llm/08.Prompt%E5%B7%A5%E7%A8%8B%E5%8C%96%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1.html" class="nav-link sidebar-link sidebar-page" aria-label="第 4 章：Prompt 的工程化结构设计"><!---->第 4 章：Prompt 的工程化结构设计<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a href="/ai-guides/tutorial/hello-llm/09.Prompt%E6%A8%A1%E6%9D%BF%E4%B8%8E%E5%B7%A5%E7%A8%8B%E6%B2%BB%E7%90%86.html" class="nav-link sidebar-link sidebar-page" aria-label="第 5 章：从 Prompt 到 Prompt 模板与工程治理"><!---->第 5 章：从 Prompt 到 Prompt 模板与工程治理<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a href="/ai-guides/tutorial/hello-llm/10.%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%20%E4%B8%8A%E4%B8%8B%E6%96%87%E4%B8%8E%E8%AE%B0%E5%BF%86.html" class="nav-link sidebar-link sidebar-page" aria-label="第三部分｜上下文与记忆：让企业知识库助手在时间维度上可靠"><!---->第三部分｜上下文与记忆：让企业知识库助手在时间维度上可靠<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a href="/ai-guides/tutorial/hello-llm/11.%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AA%97%E5%8F%A3%E7%9A%84%E7%9C%9F%E5%AE%9E%E8%BE%B9%E7%95%8C.html" class="nav-link sidebar-link sidebar-page" aria-label="第 6 章：上下文窗口的真实边界"><!---->第 6 章：上下文窗口的真实边界<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a href="/ai-guides/tutorial/hello-llm/12.%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86.html" class="nav-link sidebar-link sidebar-page" aria-label="第 7 章：从 “堆上下文” 到 “管理上下文”"><!---->第 7 章：从 “堆上下文” 到 “管理上下文”<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-current="page" href="/ai-guides/tutorial/hello-llm/13.%E8%AE%B0%E5%BF%86%E7%AD%96%E7%95%A5.html" class="router-link-active router-link-exact-active nav-link active sidebar-link sidebar-page active" aria-label="第 8 章：记忆策略的工程化选择"><!---->第 8 章：记忆策略的工程化选择<!----></a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a aria-current="page" href="/ai-guides/tutorial/hello-llm/13.%E8%AE%B0%E5%BF%86%E7%AD%96%E7%95%A5.html#_8-1-技术决策视角-记忆不是功能-而是策略组合" class="router-link-active router-link-exact-active nav-link sidebar-link heading" aria-label="8.1 技术决策视角：记忆不是功能，而是策略组合"><!---->8.1 技术决策视角：记忆不是功能，而是策略组合<!----></a><ul class="sidebar-sub-headers"></ul></li><li class="sidebar-sub-header"><a aria-current="page" href="/ai-guides/tutorial/hello-llm/13.%E8%AE%B0%E5%BF%86%E7%AD%96%E7%95%A5.html#_8-2-短期记忆-受控的上下文拼接-什么时候足够用" class="router-link-active router-link-exact-active nav-link sidebar-link heading" aria-label="8.2 短期记忆：受控的上下文拼接（什么时候足够用）"><!---->8.2 短期记忆：受控的上下文拼接（什么时候足够用）<!----></a><ul class="sidebar-sub-headers"></ul></li><li class="sidebar-sub-header"><a aria-current="page" href="/ai-guides/tutorial/hello-llm/13.%E8%AE%B0%E5%BF%86%E7%AD%96%E7%95%A5.html#_8-3-中期记忆-由-llm-维护的状态摘要-为什么要引入" class="router-link-active router-link-exact-active nav-link sidebar-link heading" aria-label="8.3 中期记忆：由 LLM 维护的状态摘要（为什么要引入）"><!---->8.3 中期记忆：由 LLM 维护的状态摘要（为什么要引入）<!----></a><ul class="sidebar-sub-headers"></ul></li><li class="sidebar-sub-header"><a aria-current="page" href="/ai-guides/tutorial/hello-llm/13.%E8%AE%B0%E5%BF%86%E7%AD%96%E7%95%A5.html#_8-4-长期记忆-为什么不能继续塞进上下文" class="router-link-active router-link-exact-active nav-link sidebar-link heading" aria-label="8.4 长期记忆：为什么不能继续塞进上下文？"><!---->8.4 长期记忆：为什么不能继续塞进上下文？<!----></a><ul class="sidebar-sub-headers"></ul></li><li class="sidebar-sub-header"><a aria-current="page" href="/ai-guides/tutorial/hello-llm/13.%E8%AE%B0%E5%BF%86%E7%AD%96%E7%95%A5.html#_8-5-短中长记忆协同工作流程" class="router-link-active router-link-exact-active nav-link sidebar-link heading" aria-label="8.5 短中长记忆协同工作流程"><!---->8.5 短中长记忆协同工作流程<!----></a><ul class="sidebar-sub-headers"></ul></li><li class="sidebar-sub-header"><a aria-current="page" href="/ai-guides/tutorial/hello-llm/13.%E8%AE%B0%E5%BF%86%E7%AD%96%E7%95%A5.html#_8-6-本章小结-记忆层次决定系统上限" class="router-link-active router-link-exact-active nav-link sidebar-link heading" aria-label="8.6 本章小结：记忆层次决定系统上限"><!---->8.6 本章小结：记忆层次决定系统上限<!----></a><ul class="sidebar-sub-headers"></ul></li></ul><!--]--></li></ul></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><span class="font-icon icon iconfont icon-write" style=""></span><span class="title">Hello Agent</span><span class="arrow end"></span></button><!----></section></li></ul><!--[--><!----><!--]--></aside><!--[--><main class="page" id="main-content"><!--[--><!----><nav class="breadcrumb disable"></nav><div class="page-title"><h1><!---->第 8 章：记忆策略的工程化选择</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://ppai.top" target="_blank" rel="noopener noreferrer">一灰灰blog</a></span><span property="author" content="一灰灰blog"></span></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><span class="page-category-item category6 clickable" role="navigation">LLM</span><meta property="articleSection" content="LLM"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><span class="page-tag-item tag6 clickable" role="navigation">LLM</span><meta property="keywords" content="LLM"></span><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2025-12-30T14:55:07.000Z"></span><!----><span class="page-word-info" aria-label="字数🔠" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon word-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="word icon"><path d="M518.217 432.64V73.143A73.143 73.143 0 01603.43 1.097a512 512 0 01419.474 419.474 73.143 73.143 0 01-72.046 85.212H591.36a73.143 73.143 0 01-73.143-73.143z"></path><path d="M493.714 566.857h340.297a73.143 73.143 0 0173.143 85.577A457.143 457.143 0 11371.566 117.76a73.143 73.143 0 0185.577 73.143v339.383a36.571 36.571 0 0036.571 36.571z"></path></svg><span>约 4321 字</span><meta property="wordCount" content="4321"></span><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 14 分钟</span><meta property="timeRequired" content="PT14M"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><div class="toc-header">此页内容<button class="print-button" title="print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/ai-guides/tutorial/hello-llm/13.%E8%AE%B0%E5%BF%86%E7%AD%96%E7%95%A5.html#_8-1-技术决策视角-记忆不是功能-而是策略组合" class="router-link-active router-link-exact-active toc-link level3">8.1 技术决策视角：记忆不是功能，而是策略组合</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/ai-guides/tutorial/hello-llm/13.%E8%AE%B0%E5%BF%86%E7%AD%96%E7%95%A5.html#_8-2-短期记忆-受控的上下文拼接-什么时候足够用" class="router-link-active router-link-exact-active toc-link level3">8.2 短期记忆：受控的上下文拼接（什么时候足够用）</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/ai-guides/tutorial/hello-llm/13.%E8%AE%B0%E5%BF%86%E7%AD%96%E7%95%A5.html#_8-3-中期记忆-由-llm-维护的状态摘要-为什么要引入" class="router-link-active router-link-exact-active toc-link level3">8.3 中期记忆：由 LLM 维护的状态摘要（为什么要引入）</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/ai-guides/tutorial/hello-llm/13.%E8%AE%B0%E5%BF%86%E7%AD%96%E7%95%A5.html#_8-4-长期记忆-为什么不能继续塞进上下文" class="router-link-active router-link-exact-active toc-link level3">8.4 长期记忆：为什么不能继续塞进上下文？</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/ai-guides/tutorial/hello-llm/13.%E8%AE%B0%E5%BF%86%E7%AD%96%E7%95%A5.html#_8-5-短中长记忆协同工作流程" class="router-link-active router-link-exact-active toc-link level3">8.5 短中长记忆协同工作流程</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/ai-guides/tutorial/hello-llm/13.%E8%AE%B0%E5%BF%86%E7%AD%96%E7%95%A5.html#_8-6-本章小结-记忆层次决定系统上限" class="router-link-active router-link-exact-active toc-link level3">8.6 本章小结：记忆层次决定系统上限</a></li><!----><!--]--></ul></div></aside></div><!----><div class="theme-hope-content"><p>在前两章中，我们已经完成了两个关键决策：</p><ul><li>否定“无限上下文”的幻想：明确上下文窗口存在硬性约束，无法承载无限制的对话信息；</li><li>接受“信息必须被分层管理”：将信息划分为不变约束、会话状态、瞬时上下文，优先保障高优先级信息的有效性。</li></ul><p>接下来，我们需要面对一个更系统级的问题，当然这也是很多小伙伴在具体落地时，最容易困惑的地方</p><blockquote><p><strong>“记忆”是否应该只有一种实现方式？</strong></p></blockquote><p>答案显然是否定的，就像人类会用“瞬时回忆”记住刚说的话、用“短期记忆”记住当天的任务、用“长期记忆”记住过往的经验一样，大模型应用的“记忆”也需要分层设计</p><p><strong>那么为什么不能用一种记忆搞定所有场景？</strong></p><blockquote><p>答：短期能用，但无法规模化、稳定化。</p><p>比如我们实现了一个公司差旅助手，初期用“全量历史+摘要”的单一记忆方式，上线后很容易出现</p><ol><li>员工跨周期咨询时，如上一次一个月前让助手定了一张机票，输入了一些关键信息，但是没有走完流程；这一次准备让助手帮忙订机票，助手可能就直接基于上一次的输入来订机票了，而员工实际上却是要去另一个地方</li><li>多员工并发咨询时，不同人的报销信息混杂，模型给出错误答复。</li></ol><p>本质是“单一记忆”无法适配“瞬时、会话、长期”三种不同的信息需求，最终导致用户体验崩塌。</p></blockquote><hr><h3 id="_8-1-技术决策视角-记忆不是功能-而是策略组合" tabindex="-1"><a class="header-anchor" href="#_8-1-技术决策视角-记忆不是功能-而是策略组合" aria-hidden="true">#</a> 8.1 技术决策视角：记忆不是功能，而是策略组合</h3><p>在很多产品讨论中，“给系统加记忆” 常被当作一个简单的功能点（比如 “让模型记住之前说过的话”）。</p><p>但从工程角度看，记忆是<strong>一组需要根据场景动态调整的策略</strong></p><p>—— 不同的信息生命周期（几秒、几小时、几天）、不同的重要性，需要匹配不同的记忆方式。</p><p>比如：</p><ul><li><em>用户当前的提问意图（“我在问报销流程”）需要 “即时记忆”，确保下一句回应不跑偏；</em></li><li><em>已确认的用户身份（“市场部员工”）需要 “会话级记忆”，在整个对话过程中保持一致；</em></li><li><em>三个月前用户咨询过的 “差旅政策” 需要 “长期记忆”，在用户再次提问时能快速关联。</em></li></ul><p>这些不同场景的需求，无法通过单一的 “记忆功能” 满足，必须设计分层的记忆策略。</p><p>下面用一张示意图，直观理解不同记忆策略的适配场景：</p><figure><img src="/ai-guides/imgs/column/llm/13-0.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><div class="mermaid-wrapper loading"><svg xmlns="http://www.w3.org/2000/svg" class="icon loading-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="loading icon"><circle cx="512" cy="512" r="0" fill="none" stroke="currentColor" stroke-width="20"><animate attributeName="r" repeatCount="indefinite" dur="1s" values="0;400" keyTimes="0;1" keySplines="0 0.2 0.8 1" calcMode="spline" begin="0s"></animate><animate attributeName="opacity" repeatCount="indefinite" dur="1s" values="1;0" keyTimes="0;1" keySplines="0.2 0 0.8 1" calcMode="spline" begin="0s"></animate></circle><circle cx="512" cy="512" r="0" fill="none" stroke="currentColor" stroke-width="20"><animate attributeName="r" repeatCount="indefinite" dur="1s" values="0;400" keyTimes="0;1" keySplines="0 0.2 0.8 1" calcMode="spline" begin="-0.333s"></animate><animate attributeName="opacity" repeatCount="indefinite" dur="1s" values="1;0" keyTimes="0;1" keySplines="0.2 0 0.8 1" calcMode="spline" begin="-0.333s"></animate></circle><circle cx="512" cy="512" r="0" fill="none" stroke="currentColor" stroke-width="20"><animate attributeName="r" repeatCount="indefinite" dur="1s" values="0;400" keyTimes="0;1" keySplines="0 0.2 0.8 1" calcMode="spline" begin="-0.667s"></animate><animate attributeName="opacity" repeatCount="indefinite" dur="1s" values="1;0" keyTimes="0;1" keySplines="0.2 0 0.8 1" calcMode="spline" begin="-0.667s"></animate></circle></svg></div><hr><h3 id="_8-2-短期记忆-受控的上下文拼接-什么时候足够用" tabindex="-1"><a class="header-anchor" href="#_8-2-短期记忆-受控的上下文拼接-什么时候足够用" aria-hidden="true">#</a> 8.2 短期记忆：受控的上下文拼接（什么时候足够用）</h3><p>最基础、成本最低的记忆策略是 “短期记忆”，适用于单一会话内的近期交互（通常持续几分钟到 1 小时）。</p><p>其核心设计是：</p><ul><li>只保留最近 N 轮对话（N 的值根据模型窗口大小确定，通常为 5-10 轮）；</li><li>明确区分 <code>system</code> / <code>state</code> / <code>recent messages</code> 三个层级，按优先级拼接上下文</li><li>Token 控制：实时统计拼接后的 Token 数量，接近窗口上限时，优先丢弃最早的瞬时对话内容，保留高优先级信息</li></ul><figure><img src="/ai-guides/imgs/column/llm/13-1.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><div class="mermaid-wrapper loading"><svg xmlns="http://www.w3.org/2000/svg" class="icon loading-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="loading icon"><circle cx="512" cy="512" r="0" fill="none" stroke="currentColor" stroke-width="20"><animate attributeName="r" repeatCount="indefinite" dur="1s" values="0;400" keyTimes="0;1" keySplines="0 0.2 0.8 1" calcMode="spline" begin="0s"></animate><animate attributeName="opacity" repeatCount="indefinite" dur="1s" values="1;0" keyTimes="0;1" keySplines="0.2 0 0.8 1" calcMode="spline" begin="0s"></animate></circle><circle cx="512" cy="512" r="0" fill="none" stroke="currentColor" stroke-width="20"><animate attributeName="r" repeatCount="indefinite" dur="1s" values="0;400" keyTimes="0;1" keySplines="0 0.2 0.8 1" calcMode="spline" begin="-0.333s"></animate><animate attributeName="opacity" repeatCount="indefinite" dur="1s" values="1;0" keyTimes="0;1" keySplines="0.2 0 0.8 1" calcMode="spline" begin="-0.333s"></animate></circle><circle cx="512" cy="512" r="0" fill="none" stroke="currentColor" stroke-width="20"><animate attributeName="r" repeatCount="indefinite" dur="1s" values="0;400" keyTimes="0;1" keySplines="0 0.2 0.8 1" calcMode="spline" begin="-0.667s"></animate><animate attributeName="opacity" repeatCount="indefinite" dur="1s" values="1;0" keyTimes="0;1" keySplines="0.2 0 0.8 1" calcMode="spline" begin="-0.667s"></animate></circle></svg></div><p>这个策略的工程价值在于：</p><ul><li><strong>行为可预测</strong>：由于只保留有限内容，token 数量可控，不会突然触发截断；</li><li><strong>成本可控</strong>：避免了全量历史导致的 token 爆炸，调用成本稳定，尤其适合中小规模应用；</li><li><strong>实现简单</strong>：无需复杂的存储或摘要逻辑，仅需维护一个滑动窗口，开发成本低、上线快。</li></ul><p>但它很快会遇到边界，这也是很多开发同学会踩的坑：</p><blockquote><p><strong>当对话跨越更长时间或主题切换频繁时，仅靠短期记忆会导致状态丢失</strong></p></blockquote><p>例如，用户上午9点咨询“年假政策”，告知助手“自己入职满3年”，助手确认“入职满3年可休10天年假”；下午2点，用户继续在同一个会话中咨询“年假是否可以拆分休”，此时短期记忆的滑动窗口已丢弃上午的对话，助手再次追问“请问您入职满几年？”，用户体验极差。</p><p>此时，就需要引入第二种记忆策略——中期记忆，来解决“会话级关键信息持久化”的问题。</p><blockquote><p><strong>高频问答：短期记忆的滑动窗口 N 怎么确定？</strong> 疑问：短期记忆的“最近N轮”，N设为5还是10？有没有统一标准？ 回复：没有统一标准，核心取决于两个因素：</p><ol><li>模型的上下文窗口大小（窗口越大，N可适当增大；如4k窗口建议N=3-5，16k窗口建议N=5-10）</li><li>单轮对话的Token长度（如果用户每轮提问都很长，N需减小，避免Token溢出） 核心原则：确保“不变约束+会话状态+最近N轮”的总Token数，不超过窗口上限的70%（预留30%给模型回应）。</li></ol></blockquote><hr><h3 id="_8-3-中期记忆-由-llm-维护的状态摘要-为什么要引入" tabindex="-1"><a class="header-anchor" href="#_8-3-中期记忆-由-llm-维护的状态摘要-为什么要引入" aria-hidden="true">#</a> 8.3 中期记忆：由 LLM 维护的状态摘要（为什么要引入）</h3><p>为了避免重要状态被挤出窗口，企业知识库助手通常会引入 “中期记忆” —— 通过“会话状态摘要”来持久化关键信息，确保在对话持续数小时或主题切换时，核心状态不丢失。</p><p>请注意，这里有一个关键误区：很多开发同学会把“会话状态摘要”和“对话内容摘要”混淆，其实两者完全不同：</p><table><thead><tr><th>类型</th><th>核心目的</th><th>内容特点</th><th>示例</th></tr></thead><tbody><tr><td>对话内容摘要</td><td>总结对话的整体内容，方便用户回顾</td><td>非结构化，侧重“过程描述”</td><td>用户咨询差旅报销，先问了高铁，再问了住宿，助手告知了相关政策。</td></tr><tr><td>会话状态摘要</td><td>持久化关键决策信息，供模型后续参考</td><td>结构化，侧重“结果确认”</td><td>用户：张三（市场部，入职3年）；已确认：高铁二等座可报销、一线城市住宿上限800元/晚。</td></tr></tbody></table><p>简单来说会话状态摘要的核心不是 “总结对话内容”，而是<strong>记录 “已确认的决策信息”</strong>，格式通常是结构化的键值对或列表。</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>【会话状态摘要】
- 当前用户：张三（市场部，入职时间2021年3月）
- 讨论主题：2025年新版差旅报销政策
- 已确认事实：
  1. 国内差旅住宿上限为一线城市800元/晚，二线600元/晚
  2. 高铁二等座可全额报销，一等座需部门负责人审批
- 待确认问题：
  1. 海外差旅是否适用此政策？
  2. 报销时效是否仍为3个月？
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这类摘要的维护机制通常是：<strong>每轮对话结束后，调用 LLM 对比新内容与当前摘要，自动更新关键信息</strong>（新增确认项、移除已解决问题、修正错误）。</p><p>这个流程可以通过示意图直观理解：</p><figure><img src="/ai-guides/imgs/column/llm/13-2.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><div class="mermaid-wrapper loading"><svg xmlns="http://www.w3.org/2000/svg" class="icon loading-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="loading icon"><circle cx="512" cy="512" r="0" fill="none" stroke="currentColor" stroke-width="20"><animate attributeName="r" repeatCount="indefinite" dur="1s" values="0;400" keyTimes="0;1" keySplines="0 0.2 0.8 1" calcMode="spline" begin="0s"></animate><animate attributeName="opacity" repeatCount="indefinite" dur="1s" values="1;0" keyTimes="0;1" keySplines="0.2 0 0.8 1" calcMode="spline" begin="0s"></animate></circle><circle cx="512" cy="512" r="0" fill="none" stroke="currentColor" stroke-width="20"><animate attributeName="r" repeatCount="indefinite" dur="1s" values="0;400" keyTimes="0;1" keySplines="0 0.2 0.8 1" calcMode="spline" begin="-0.333s"></animate><animate attributeName="opacity" repeatCount="indefinite" dur="1s" values="1;0" keyTimes="0;1" keySplines="0.2 0 0.8 1" calcMode="spline" begin="-0.333s"></animate></circle><circle cx="512" cy="512" r="0" fill="none" stroke="currentColor" stroke-width="20"><animate attributeName="r" repeatCount="indefinite" dur="1s" values="0;400" keyTimes="0;1" keySplines="0 0.2 0.8 1" calcMode="spline" begin="-0.667s"></animate><animate attributeName="opacity" repeatCount="indefinite" dur="1s" values="1;0" keyTimes="0;1" keySplines="0.2 0 0.8 1" calcMode="spline" begin="-0.667s"></animate></circle></svg></div><p>它的核心价值是：<strong>将分散在多轮对话中的关键信息 “浓缩固化”，避免被短期记忆的滑动窗口 “挤出”</strong>。</p><p>即使短期记忆只保留最近 5 轮，状态摘要也能确保 “用户身份”“已确认规则” 等信息持续影响模型决策。</p><blockquote><p>这不是对话摘要，而是<strong>决策状态的持久化</strong>。</p></blockquote><hr><h3 id="_8-4-长期记忆-为什么不能继续塞进上下文" tabindex="-1"><a class="header-anchor" href="#_8-4-长期记忆-为什么不能继续塞进上下文" aria-hidden="true">#</a> 8.4 长期记忆：为什么不能继续塞进上下文？</h3><p>当对话场景扩展到以下情况时，仅靠短期和中期记忆（依赖上下文窗口）就会变得不现实——它们的核心局限是“无法脱离上下文窗口存在”，而窗口容量和单一会话的限制，决定了它们无法应对“跨时间、跨规模”的记忆需求：</p><ul><li><strong>跨天对话</strong>：用户今天咨询一半，明天继续（此时会话可能已被重置，中期记忆的摘要也会丢失）；</li><li><strong>多任务切换</strong>：用户同时处理 “报销”“年假”“绩效” 多个主题（每个主题有独立的关键信息，塞进同一个上下文会导致信息混杂，模型混淆）；</li><li><strong>多用户并发</strong>：同一个助手服务多个员工，需要区分每个人的历史。</li></ul><p>此时，继续将所有信息塞进上下文窗口会导致两个问题：</p><ol><li>窗口容量有限，无法容纳多天的状态摘要；</li><li>不同用户 / 任务的信息混杂，导致模型混淆。</li></ol><p>因此，我们需要一种 “移出上下文但可按需召回” 的记忆机制 —— 这正是<strong>长期记忆</strong>的核心价值。</p><p>长期记忆的实现通常依赖外部存储（如数据库、向量数据库），其核心逻辑是：</p><ul><li>将不活跃的会话状态（如 24 小时未更新）从上下文移出，存入外部存储；</li><li>当用户再次激活对话时，通过检索（如基于用户 ID、主题关键词）将相关历史状态重新导入上下文。</li></ul><p>以企业助手为例，具体的实现步骤通常有下面四步：</p><ol><li><p>存储：将不活跃的会话状态（如 24 小时未更新的会话）、用户历史咨询记录，从上下文移出，存入外部存储（向量数据库优先，方便后续关键词检索）；存储时需关联“用户ID、主题关键词、时间戳”，便于后续召回；</p></li><li><p>检索：当用户再次激活对话（如第二天继续咨询）或切换主题时，模型先提取当前对话的“用户ID、主题关键词”（比如“张三、海外差旅”），通过检索工具查询外部存储，找到相关的历史记忆；</p></li><li><p>召回：将检索到的历史记忆（如张三上次未完成的“海外差旅政策”咨询记录、已确认的“入职年限”），精简后重新导入上下文，结合短期、中期记忆，生成回应；</p></li><li><p>更新：本轮对话结束后，更新长期记忆中的相关记录，确保历史信息的时效性（比如补充新确认的“海外差旅住宿上限”）。</p></li></ol><blockquote><p><strong>长期记忆的核心不是 “记住一切”，而是 “在需要时，能把正确的信息带回上下文”</strong> —— 它解决的是 “跨时间、跨规模” 的记忆问题。</p></blockquote><p>此时，一个新的技术出现了：这正是<code>向量检索</code>、<code>RAG</code>等机制存在的根本原因。</p><h3 id="_8-5-短中长记忆协同工作流程" tabindex="-1"><a class="header-anchor" href="#_8-5-短中长记忆协同工作流程" aria-hidden="true">#</a> 8.5 短中长记忆协同工作流程</h3><p>在实际的项目工程中，上面提到的短期、中期、长期三种记忆策略通常不是独立使用的、而是协同工作，形成完整的记忆体系，下面是一个典型的示意图</p><figure><img src="/ai-guides/imgs/column/llm/13-3.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><div class="mermaid-wrapper loading"><svg xmlns="http://www.w3.org/2000/svg" class="icon loading-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="loading icon"><circle cx="512" cy="512" r="0" fill="none" stroke="currentColor" stroke-width="20"><animate attributeName="r" repeatCount="indefinite" dur="1s" values="0;400" keyTimes="0;1" keySplines="0 0.2 0.8 1" calcMode="spline" begin="0s"></animate><animate attributeName="opacity" repeatCount="indefinite" dur="1s" values="1;0" keyTimes="0;1" keySplines="0.2 0 0.8 1" calcMode="spline" begin="0s"></animate></circle><circle cx="512" cy="512" r="0" fill="none" stroke="currentColor" stroke-width="20"><animate attributeName="r" repeatCount="indefinite" dur="1s" values="0;400" keyTimes="0;1" keySplines="0 0.2 0.8 1" calcMode="spline" begin="-0.333s"></animate><animate attributeName="opacity" repeatCount="indefinite" dur="1s" values="1;0" keyTimes="0;1" keySplines="0.2 0 0.8 1" calcMode="spline" begin="-0.333s"></animate></circle><circle cx="512" cy="512" r="0" fill="none" stroke="currentColor" stroke-width="20"><animate attributeName="r" repeatCount="indefinite" dur="1s" values="0;400" keyTimes="0;1" keySplines="0 0.2 0.8 1" calcMode="spline" begin="-0.667s"></animate><animate attributeName="opacity" repeatCount="indefinite" dur="1s" values="1;0" keyTimes="0;1" keySplines="0.2 0 0.8 1" calcMode="spline" begin="-0.667s"></animate></circle></svg></div><hr><h3 id="_8-6-本章小结-记忆层次决定系统上限" tabindex="-1"><a class="header-anchor" href="#_8-6-本章小结-记忆层次决定系统上限" aria-hidden="true">#</a> 8.6 本章小结：记忆层次决定系统上限</h3><p>通过本章的分析，我们可以明确：企业知识库助手的 “记忆” 是分层的策略组合，不同层次解决不同问题，三者协同，才能构建稳定、高效、用户体验好的记忆体系：</p><ul><li>短期记忆：通过受控的上下文拼接，解决 “当前几轮对话的连贯性”；</li><li>中期记忆：通过状态摘要，解决 “会话级关键信息的一致性”；</li><li>长期记忆：通过外部存储与检索，解决 “跨时间、跨任务的信息召回”。</li></ul><p>这三层记忆共同构成了系统在时间维度上的可靠性基础。但还有一个关键问题尚未解决：</p><blockquote><p><em>当用户的问题超出模型自身的知识范围（比如最新的公司政策），仅靠记忆策略如何保证回答准确？</em></p></blockquote><p>而这自然引出了下一部分的讨论主题：我们将深入探讨 RAG（检索增强生成）技术如何与上下文工程结合，让系统的知识边界可控、可扩展。</p><p><strong>扩展思考</strong></p><ol><li><p>你所开发的 LLM 应用，会话持续时间通常是多久？如果是跨天对话，长期记忆的“不活跃阈值”（如24小时）如何设定更合理？</p></li><li><p>中期记忆的会话状态摘要，每轮都调用 LLM 更新会增加成本，如何设计“更新触发条件”（比如仅当有新的确认项时才更新），平衡成本与效果？</p></li><li><p>如果没有向量数据库，能否用“关键词匹配”替代向量检索，实现长期记忆的召回？这种方式的局限性是什么？</p></li></ol></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/liuyueyi/ai-guides/edit/main/src/tutorial/hello-llm/13.记忆策略.md" rel="noopener noreferrer" target="_blank" aria-label="在 GitHub 上编辑此页" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub 上编辑此页<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><div class="contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: bangzewu@126.com">yihui</span><!--]--><!--]--></div></div></footer><nav class="page-nav"><a href="/ai-guides/tutorial/hello-llm/12.%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86.html" class="nav-link prev" aria-label="第 7 章：从 “堆上下文” 到 “管理上下文”"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->第 7 章：从 “堆上下文” 到 “管理上下文”</div></a><!----></nav><div class="giscus-wrapper input-top" id="comment" style="display:block;"><div style="text-align:center">Loading...</div></div><!----><!--]--></main><!--]--><footer class="footer-wrapper"><div class="footer">鄂ICP备18017282号</div><div class="copyright">Copyright © 2026 一灰灰blog</div></footer></div><!--]--><!----><!----><!--]--></div>
    <script type="module" src="/ai-guides/assets/app-f3f5ea18.js" defer></script>
  </body>
</html>
